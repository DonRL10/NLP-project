{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8a2cf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d269a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"surrey-nlp/PLOD-CW-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16c63684",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"O\", \"B-AC\", \"B-LF\", \"I-LF\"]\n",
    "n_labels = len(labels)\n",
    "ltoi = {l: i for i, l in enumerate(labels)}\n",
    "itol = {i: l for l, i in ltoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac37730f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/HS400/rl01179/.local/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"microsoft/deberta-v3-base\" # Or deberta-v3-large, etc.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Check if the tokenizer is a fast tokenizer (it should be for DeBERTa-v3)\n",
    "assert tokenizer.is_fast, \"Only fast tokenizers are supported for this example.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c201d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    aligned_labels_batch = []\n",
    "    for i, label_sequence in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids_for_sequence = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:  # Special tokens ([CLS], [SEP]) or padding\n",
    "                label_ids_for_sequence.append(ltoi[\"O\"]) # Assign 'O', will be masked by CRF if padding\n",
    "            elif word_idx != previous_word_idx:  # First token of a new word\n",
    "                label_ids_for_sequence.append(ltoi[label_sequence[word_idx]])\n",
    "            else:  # Subsequent tokens of the same word\n",
    "                current_label_str = label_sequence[word_idx]\n",
    "                # Propagate I-tag if B-tag, otherwise keep current tag (O or I-tag)\n",
    "                if current_label_str.startswith(\"B-\"):\n",
    "                    related_i_tag = \"I-\" + current_label_str[2:]\n",
    "                    if related_i_tag in ltoi:\n",
    "                        label_ids_for_sequence.append(ltoi[related_i_tag])\n",
    "                    else: # No specific I-tag, e.g. B-AC without I-AC\n",
    "                        label_ids_for_sequence.append(ltoi[current_label_str]) # or ltoi[\"O\"]\n",
    "                else:\n",
    "                    label_ids_for_sequence.append(ltoi[current_label_str])\n",
    "            previous_word_idx = word_idx\n",
    "        aligned_labels_batch.append(label_ids_for_sequence)\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels_batch\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5041678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc9fe94d16d41f3a5b03668f3a9168f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64785b90ead647f09306d5576af11c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674f3f8312584c1fa5dcb06a393ed43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = dataset.map(tokenize_and_align_labels, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6704a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, train_attention_mask = data['train']['input_ids'], data['train']['labels'], data['train']['attention_mask']\n",
    "val_data, val_labels, val_attention_mask = data['validation']['input_ids'], data['validation']['labels'], data['validation']['attention_mask']\n",
    "test_data, test_labels, test_attention_mask = data['test']['input_ids'], data['test']['labels'], data['test']['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9eb4dccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 8\n",
    "\n",
    "def get_batch(split = \"train\"):\n",
    "  data = train_data if split == \"train\" else val_data\n",
    "  labels = train_labels if split == \"train\" else val_labels\n",
    "  attention_mask = train_attention_mask if split == \"train\" else val_attention_mask\n",
    "  ix = torch.randint(len(data), (batch_size,))\n",
    "  x = torch.stack([torch.tensor(data[i]).long() for i in ix])\n",
    "  y = torch.stack([torch.tensor(labels[i]).long() for i in ix])\n",
    "  a = torch.stack([torch.tensor(attention_mask[i]) for i in ix])\n",
    "  return x.to(device), y.to(device), a.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(eval_steps):\n",
    "  out = {}\n",
    "  model.eval()\n",
    "  for split in [\"train\", \"validation\"]:\n",
    "    losses = torch.zeros(eval_steps)\n",
    "    for k in range(eval_steps):\n",
    "      x, y, a = get_batch(split)\n",
    "      loss, _ = model(x, attention_mask = a, labels = y)\n",
    "      losses[k] = loss.item()\n",
    "    out[split] = losses.mean()\n",
    "  model.train()\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7fa5c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "from torchcrf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74f826ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebertaCrfForTokenClassification(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_labels, id2label, label2id):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.deberta = AutoModelForTokenClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels, # DeBERTa's classifier head outputs scores for each label\n",
    "            id2label=id2label,\n",
    "            label2id=label2id\n",
    "        )\n",
    "        self.crf = CRF(num_tags=num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        emissions = outputs.logits # Shape: (batch_size, seq_length, num_labels)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = -self.crf(emissions, labels, mask=attention_mask.bool(), reduction='mean')\n",
    "            return loss, emissions\n",
    "        else:\n",
    "\n",
    "            decoded_tags = self.crf.decode(emissions, mask=attention_mask.bool())\n",
    "            return decoded_tags, emissions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd5fd18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DebertaCrfForTokenClassification(\n",
    "    model_checkpoint,\n",
    "    num_labels=n_labels,\n",
    "    id2label=itol,\n",
    "    label2id=ltoi\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5eee2863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr = 1e-5)\n",
    "max_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c85f2f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [01:27<24:20:10, 87.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 63.5596, val loss 64.2739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 101/1000 [03:39<6:43:25, 26.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100: train loss 14.3790, val loss 16.6687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 201/1000 [05:51<5:58:47, 26.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200: train loss 11.9993, val loss 14.0732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 301/1000 [08:02<5:14:00, 26.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 300: train loss 9.2189, val loss 12.3455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 401/1000 [10:14<4:29:19, 26.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400: train loss 8.1032, val loss 12.6389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 501/1000 [12:26<3:44:28, 26.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 7.9043, val loss 12.9673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 601/1000 [14:38<2:59:19, 26.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 600: train loss 7.3925, val loss 12.5551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 701/1000 [16:50<2:14:36, 27.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 700: train loss 8.0750, val loss 12.9865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 800/1000 [17:47<04:26,  1.33s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3236294/2708739351.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m# lossi.append(loss.item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step {step}: train loss {losses['train']:.4f}, val loss {losses['validation']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3236294/2848704959.py\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m(eval_steps)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m       \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3236294/475502641.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memissions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memissions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torchcrf/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, emissions, tags, mask, reduction)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mreduction\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mnone\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \"\"\"\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memissions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'token_mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'invalid reduction: {reduction}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torchcrf/__init__.py\u001b[0m in \u001b[0;36m_validate\u001b[0;34m(self, emissions, tags, mask)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mno_empty_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mno_empty_seq_bf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_empty_seq\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_empty_seq_bf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mask of the first timestep must all be on'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lossi = []\n",
    "lri = []\n",
    "\n",
    "for step in tqdm(range(max_steps)):\n",
    "  # for g in optim.param_groups:\n",
    "  #   g['lr'] = lrs[step]\n",
    "\n",
    "  x, y, a = get_batch(\"train\")\n",
    "  optim.zero_grad()\n",
    "  with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "    loss, logits = model(x, attention_mask = a, labels = y)\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "  # lri.append(lre[step])\n",
    "  # lossi.append(loss.item())\n",
    "  if step % 100 == 0:\n",
    "    losses = estimate_loss(200)\n",
    "    print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['validation']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "856be4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, recall_score, precision_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a774f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model_crf(split=\"test\", current_model=None, current_device=None,\n",
    "                       current_itol=None,\n",
    "                       current_test_data=None, current_test_labels=None, current_test_attention_mask=None,\n",
    "                       current_val_data=None, current_val_labels=None, current_val_attention_mask=None,\n",
    "                       current_train_data=None, current_train_labels=None, current_train_attention_mask=None):\n",
    "    \"\"\"Evaluate CRF model performance on given split with seqeval metrics\"\"\"\n",
    "    if current_model is None: global model\n",
    "    else: model = current_model # Use passed model if provided\n",
    "\n",
    "    if current_device is None: global device\n",
    "    else: device = current_device\n",
    "\n",
    "    if current_itol is None: global itol\n",
    "    else: itol = current_itol\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Select data based on split\n",
    "    # Using passed-in data if available, otherwise global\n",
    "    if split == \"test\":\n",
    "        data_input_ids = current_test_data if current_test_data is not None else test_data\n",
    "        data_labels = current_test_labels if current_test_labels is not None else test_labels\n",
    "        data_attention_mask = current_test_attention_mask if current_test_attention_mask is not None else test_attention_mask\n",
    "    elif split == \"validation\":\n",
    "        data_input_ids = current_val_data if current_val_data is not None else val_data\n",
    "        data_labels = current_val_labels if current_val_labels is not None else val_labels\n",
    "        data_attention_mask = current_val_attention_mask if current_val_attention_mask is not None else val_attention_mask\n",
    "    elif split == \"train\":\n",
    "        data_input_ids = current_train_data if current_train_data is not None else train_data\n",
    "        data_labels = current_train_labels if current_train_labels is not None else train_labels\n",
    "        data_attention_mask = current_train_attention_mask if current_train_attention_mask is not None else train_attention_mask\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid split: {split}. Choose from 'train', 'validation', 'test'.\")\n",
    "\n",
    "    # Process in smaller batches to avoid OOM\n",
    "    batch_size_eval = 16  # You can adjust this\n",
    "    all_true_labels_str = []\n",
    "    all_pred_labels_str = []\n",
    "\n",
    "    # Process the entire dataset for the chosen split\n",
    "    for i in tqdm(range(0, len(data_input_ids), batch_size_eval), desc=f\"Evaluating on {split} with CRF\"):\n",
    "        # Get batch by slicing the pre-loaded lists of lists\n",
    "        batch_input_ids_list = data_input_ids[i:i+batch_size_eval]\n",
    "        batch_labels_list = data_labels[i:i+batch_size_eval]\n",
    "        batch_attention_mask_list = data_attention_mask[i:i+batch_size_eval]\n",
    "\n",
    "        # Convert to tensors\n",
    "        batch_input_ids = torch.tensor(batch_input_ids_list, dtype=torch.long).to(device)\n",
    "        batch_labels_ids = torch.tensor(batch_labels_list, dtype=torch.long).to(device) # True labels\n",
    "        batch_attention_mask = torch.tensor(batch_attention_mask_list, dtype=torch.uint8).to(device)\n",
    "\n",
    "        # Get predictions from CRF model (no labels passed for inference)\n",
    "        # The CRF model's forward pass (without labels) returns (decoded_tags, emissions)\n",
    "        if device.type == 'cuda':\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16):\n",
    "                predicted_tags_batch, _ = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "        else:\n",
    "            predicted_tags_batch, _ = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "        # predicted_tags_batch is a list of lists of predicted tag INDICES from crf.decode()\n",
    "\n",
    "        # Convert predictions and true labels to string lists for seqeval\n",
    "        for j in range(batch_input_ids.size(0)): # Iterate over each sample in the batch\n",
    "            true_label_ids_for_sample = batch_labels_ids[j].cpu().tolist()\n",
    "            # predicted_tags_batch[j] is already a list of python ints (tag indices)\n",
    "            pred_label_ids_for_sample = predicted_tags_batch[j]\n",
    "            current_sample_mask = batch_attention_mask[j].cpu().tolist()\n",
    "\n",
    "            true_seq_str = []\n",
    "            pred_seq_str = []\n",
    "\n",
    "            for k in range(len(true_label_ids_for_sample)): # Iterate over tokens in the sequence\n",
    "                if current_sample_mask[k] == 1:  # Only consider active tokens based on attention mask\n",
    "                    true_seq_str.append(itol[true_label_ids_for_sample[k]])\n",
    "                    pred_seq_str.append(itol[pred_label_ids_for_sample[k]])\n",
    "                # else: # This token is padding, so we stop or ignore\n",
    "                #     If we want to cut sequences at the first padding:\n",
    "                #     if len(true_seq_str) > 0: # ensure we are not at the beginning of padding\n",
    "                #         break\n",
    "\n",
    "            if true_seq_str:  # Only add if the sequence (after masking) is not empty\n",
    "                all_true_labels_str.append(true_seq_str)\n",
    "                all_pred_labels_str.append(pred_seq_str)\n",
    "\n",
    "    # Calculate metrics using seqeval\n",
    "    # Ensure that all_true_labels_str and all_pred_labels_str are not empty\n",
    "    if not all_true_labels_str or not all_pred_labels_str:\n",
    "        print(f\"No data to evaluate for split {split}. True labels count: {len(all_true_labels_str)}, Pred labels count: {len(all_pred_labels_str)}\")\n",
    "        return {\n",
    "            \"f1\": 0.0,\n",
    "            \"precision\": 0.0,\n",
    "            \"recall\": 0.0,\n",
    "            \"report\": \"No data to evaluate.\"\n",
    "        }\n",
    "\n",
    "    precision = precision_score(all_true_labels_str, all_pred_labels_str)\n",
    "    recall = recall_score(all_true_labels_str, all_pred_labels_str)\n",
    "    f1 = f1_score(all_true_labels_str, all_pred_labels_str)\n",
    "    report = classification_report(all_true_labels_str, all_pred_labels_str, digits=4)\n",
    "\n",
    "    print(f\"\\n=== CRF Model Evaluation on {split} split ===\")\n",
    "    print(f\"F1 Score (overall): {f1:.4f}\")\n",
    "    print(f\"Precision (overall): {precision:.4f}\")\n",
    "    print(f\"Recall (overall): {recall:.4f}\")\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    model.train() # Set model back to training mode\n",
    "\n",
    "    return {\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"report\": report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5a59b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on test with CRF: 100%|██████████| 16/16 [00:02<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CRF Model Evaluation on test split ===\n",
      "F1 Score (overall): 0.8634\n",
      "Precision (overall): 0.8474\n",
      "Recall (overall): 0.8799\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AC     0.8799    0.8733    0.8766      1342\n",
      "          LF     0.7705    0.8983    0.8295       482\n",
      "\n",
      "   micro avg     0.8474    0.8799    0.8634      1824\n",
      "   macro avg     0.8252    0.8858    0.8530      1824\n",
      "weighted avg     0.8510    0.8799    0.8641      1824\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.8633674018289403,\n",
       " 'precision': 0.8474128827877508,\n",
       " 'recall': 0.8799342105263158,\n",
       " 'report': '              precision    recall  f1-score   support\\n\\n          AC     0.8799    0.8733    0.8766      1342\\n          LF     0.7705    0.8983    0.8295       482\\n\\n   micro avg     0.8474    0.8799    0.8634      1824\\n   macro avg     0.8252    0.8858    0.8530      1824\\nweighted avg     0.8510    0.8799    0.8641      1824\\n'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model_crf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a644c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
