{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e962acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed5eeba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/datasets/surrey-nlp/PLOD-CW-25/resolve/main/README.md\n",
      "Retrying in 1s [Retry 1/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/datasets/surrey-nlp/PLOD-CW-25/resolve/main/README.md\n",
      "Retrying in 2s [Retry 2/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/datasets/surrey-nlp/PLOD-CW-25/resolve/main/README.md\n",
      "Retrying in 4s [Retry 3/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/datasets/surrey-nlp/PLOD-CW-25/resolve/main/README.md\n",
      "Retrying in 8s [Retry 4/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/datasets/surrey-nlp/PLOD-CW-25/resolve/main/README.md\n",
      "Retrying in 8s [Retry 5/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/datasets/surrey-nlp/PLOD-CW-25/resolve/main/README.md\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/datasets/surrey-nlp/PLOD-CW-25/resolve/9e3083d6df56ff798c62bd3fe0bad61295c67e3c/PLOD-CW-25.py\n",
      "Retrying in 1s [Retry 1/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/datasets/surrey-nlp/PLOD-CW-25/resolve/9e3083d6df56ff798c62bd3fe0bad61295c67e3c/PLOD-CW-25.py\n",
      "Retrying in 2s [Retry 2/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/datasets/surrey-nlp/PLOD-CW-25/resolve/9e3083d6df56ff798c62bd3fe0bad61295c67e3c/PLOD-CW-25.py\n",
      "Retrying in 4s [Retry 3/5].\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"surrey-nlp/PLOD-CW-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "518209f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"O\", \"B-AC\", \"B-LF\", \"I-LF\"]\n",
    "n_labels = len(labels)\n",
    "ltoi = {l: i for i, l in enumerate(labels)}\n",
    "itol = {i: l for l, i in ltoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb97fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb811fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeds(examples):\n",
    "    encoded_input = tokenizer(examples['tokens'], padding=True, truncation=True, return_tensors='pt', is_split_into_words=True)\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1b4edae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True, # Crucial for pre-tokenized input\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label_sequence in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None: # Special tokens ([CLS], [SEP])\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx: # First token of a new word\n",
    "                label_ids.append(ltoi[label_sequence[word_idx]])\n",
    "            else: # Subsequent tokens of the same word\n",
    "                label_ids.append(-100)\n",
    "\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f0ef6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f17eaa76aa4b23b609a9fb7c525872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = dataset.map(tokenize_and_align_labels, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54caf7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, train_attention_mask = data['train']['input_ids'], data['train']['labels'], data['train']['attention_mask']\n",
    "val_data, val_labels, val_attention_mask = data['validation']['input_ids'], data['validation']['labels'], data['validation']['attention_mask']\n",
    "test_data, test_labels, test_attention_mask = data['test']['input_ids'], data['test']['labels'], data['test']['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e83db9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 8\n",
    "\n",
    "def get_batch(split = \"train\"):\n",
    "  data = train_data if split == \"train\" else val_data\n",
    "  labels = train_labels if split == \"train\" else val_labels\n",
    "  attention_mask = train_attention_mask if split == \"train\" else val_attention_mask\n",
    "  ix = torch.randint(len(data), (batch_size,))\n",
    "  x = torch.stack([torch.tensor(data[i]).long() for i in ix])\n",
    "  y = torch.stack([torch.tensor(labels[i]).long() for i in ix])\n",
    "  a = torch.stack([torch.tensor(attention_mask[i]) for i in ix])\n",
    "  return x.to(device), y.to(device), a.to(device)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(eval_steps):\n",
    "  out = {}\n",
    "  model_ffn.eval()\n",
    "  for split in [\"train\", \"validation\"]:\n",
    "    losses = torch.zeros(eval_steps)\n",
    "    for k in range(eval_steps):\n",
    "      x, y, a = get_batch(split)\n",
    "      logits = model_ffn(x, attention_mask = a)\n",
    "      loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), y.view(-1))\n",
    "      losses[k] = loss.item()\n",
    "    out[split] = losses.mean()\n",
    "  model_ffn.train()\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e40c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "242e0178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniLM_FFN(nn.Module):\n",
    "    def __init__(self, model, dim, n_labels):\n",
    "        super().__init__()\n",
    "        self.minilm = model\n",
    "        self.n_emb = model.config.hidden_size # Should be 384\n",
    "\n",
    "        \n",
    "        for param in self.minilm.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.lstm = nn.LSTM(self.n_emb, dim, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.proj = nn.Linear(dim * 2, n_labels)\n",
    "\n",
    "    def forward(self, idxs, attention_mask):\n",
    "        # Get MiniLM embeddings\n",
    "        # outputs.last_hidden_state shape: (batch_size, seq_len, minilm_embedding_dim)\n",
    "        x = self.minilm(input_ids=idxs, attention_mask=attention_mask)\n",
    "        tok_embs = x.last_hidden_state\n",
    "\n",
    "        # Apply FFN\n",
    "        logits = self.proj(self.lstm(tok_embs)[0])\n",
    "    \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c560e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e63727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ffn = MiniLM_FFN(model, 256, 4).to(device)\n",
    "model_ffn = torch.compile(model_ffn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5249a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "optim = torch.optim.AdamW(model_ffn.parameters(), lr = 1e-3)\n",
    "max_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d72de386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1000 [00:08<14:31,  1.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.3146, val loss 1.3137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 511/1000 [00:25<02:29,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 0.2287, val loss 0.3506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:34<00:00, 28.94it/s]\n"
     ]
    }
   ],
   "source": [
    "lossi = []\n",
    "lri = []\n",
    "\n",
    "for step in tqdm(range(max_steps)):\n",
    "  # for g in optim.param_groups:\n",
    "  #   g['lr'] = lrs[step]\n",
    "\n",
    "  x, y, a = get_batch(\"train\")\n",
    "  optim.zero_grad()\n",
    "  with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "    logits = model_ffn(x, attention_mask = a)\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), y.view(-1))\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "  # lri.append(lre[step])\n",
    "  # lossi.append(loss.item())\n",
    "  if step % 500 == 0:\n",
    "    losses = estimate_loss(200)\n",
    "    print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['validation']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "881438e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4600fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(split=\"test\"):\n",
    "    \"\"\"Evaluate model performance on given split with seqeval metrics\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    if split == \"test\":\n",
    "        data_input_ids = test_data\n",
    "        data_labels = test_labels\n",
    "        data_attention_mask = test_attention_mask\n",
    "    elif split == \"validation\":\n",
    "        data_input_ids = val_data\n",
    "        data_labels = val_labels\n",
    "        data_attention_mask = val_attention_mask\n",
    "    else:\n",
    "        data_input_ids = train_data\n",
    "        data_labels = train_labels\n",
    "        data_attention_mask = train_attention_mask\n",
    "    \n",
    "    # Process in smaller batches to avoid OOM\n",
    "    batch_size_eval = 16\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "    \n",
    "    # Process the entire dataset\n",
    "    for i in tqdm(range(0, len(data_input_ids), batch_size_eval), desc=f\"Evaluating on {split}\"):\n",
    "        # Get batch\n",
    "        batch_input_ids = torch.tensor(data_input_ids[i:i+batch_size_eval]).to(device)\n",
    "        batch_labels = torch.tensor(data_labels[i:i+batch_size_eval]).to(device)\n",
    "        batch_attention_mask = torch.tensor(data_attention_mask[i:i+batch_size_eval]).to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        logits = model_ffn(batch_input_ids, attention_mask=batch_attention_mask)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Convert predictions and labels to lists for seqeval\n",
    "        for j in range(len(batch_input_ids)):\n",
    "            true_label_ids = batch_labels[j].cpu().numpy()\n",
    "            pred_label_ids = predictions[j].cpu().numpy()\n",
    "            \n",
    "            # Convert IDs to labels, handling special tokens\n",
    "            true_seq = []\n",
    "            pred_seq = []\n",
    "            \n",
    "            for true_id, pred_id, mask in zip(true_label_ids, pred_label_ids, batch_attention_mask[j]):\n",
    "                if mask == 1 and true_id != -100:  # Only evaluate on non-padding and non-special tokens\n",
    "                    true_seq.append(itol[true_id.item()])\n",
    "                    pred_seq.append(itol[pred_id.item()])\n",
    "            \n",
    "            if true_seq:  # Only add if not empty\n",
    "                all_true_labels.append(true_seq)\n",
    "                all_pred_labels.append(pred_seq)\n",
    "    \n",
    "    # Calculate metrics using seqeval\n",
    "    precision = precision_score(all_true_labels, all_pred_labels)\n",
    "    recall = recall_score(all_true_labels, all_pred_labels)\n",
    "    f1 = f1_score(all_true_labels, all_pred_labels)\n",
    "    report = classification_report(all_true_labels, all_pred_labels)\n",
    "    \n",
    "    print(f\"\\n=== Evaluation on {split} split ===\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    return {\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"report\": report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37836d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on test: 100%|██████████| 16/16 [00:04<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on test split ===\n",
      "F1 Score: 0.7381\n",
      "Precision: 0.7253\n",
      "Recall: 0.7514\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AC       0.83      0.82      0.82       797\n",
      "          LF       0.58      0.64      0.60       482\n",
      "\n",
      "   micro avg       0.73      0.75      0.74      1279\n",
      "   macro avg       0.70      0.73      0.71      1279\n",
      "weighted avg       0.73      0.75      0.74      1279\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.7380952380952381,\n",
       " 'precision': 0.7252830188679246,\n",
       " 'recall': 0.7513682564503519,\n",
       " 'report': '              precision    recall  f1-score   support\\n\\n          AC       0.83      0.82      0.82       797\\n          LF       0.58      0.64      0.60       482\\n\\n   micro avg       0.73      0.75      0.74      1279\\n   macro avg       0.70      0.73      0.71      1279\\nweighted avg       0.73      0.75      0.74      1279\\n'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec8b5865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on validation: 100%|██████████| 10/10 [00:01<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on validation split ===\n",
      "F1 Score: 0.6911\n",
      "Precision: 0.6765\n",
      "Recall: 0.7064\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AC       0.81      0.77      0.79       508\n",
      "          LF       0.50      0.59      0.54       306\n",
      "\n",
      "   micro avg       0.68      0.71      0.69       814\n",
      "   macro avg       0.65      0.68      0.67       814\n",
      "weighted avg       0.69      0.71      0.70       814\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.6911057692307693,\n",
       " 'precision': 0.6764705882352942,\n",
       " 'recall': 0.7063882063882064,\n",
       " 'report': '              precision    recall  f1-score   support\\n\\n          AC       0.81      0.77      0.79       508\\n          LF       0.50      0.59      0.54       306\\n\\n   micro avg       0.68      0.71      0.69       814\\n   macro avg       0.65      0.68      0.67       814\\nweighted avg       0.69      0.71      0.70       814\\n'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model('validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f6e52b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
