{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Skt_i73Mz32B",
        "outputId": "d9c613fa-889e-45ba-e744-0c3b5cbeec70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /user/HS400/rl01179/.local/lib/python3.10/site-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /user/HS400/rl01179/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /user/HS400/rl01179/.local/lib/python3.10/site-packages (from datasets) (0.31.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
            "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /user/HS400/rl01179/.local/lib/python3.10/site-packages (from datasets) (2025.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Collecting pyarrow>=15.0.0\n",
            "  Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess<0.70.17\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /user/HS400/rl01179/.local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (1.26.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2020.6.20)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.8/219.8 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.17.0\n",
            "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.9/333.9 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Collecting aiohappyeyeballs>=2.3.0\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.3/287.3 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting propcache>=0.2.0\n",
            "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout<6.0,>=4.0\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, pyarrow, propcache, multidict, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.6.0 dill-0.3.8 frozenlist-1.6.0 multidict-6.4.3 multiprocess-0.70.16 propcache-0.3.1 pyarrow-20.0.0 xxhash-3.5.0 yarl-1.20.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OyYDA2N5jaS3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LotoArojjnG0",
        "outputId": "ef77bc27-76f8-4eaf-b749-162c27966662"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"surrey-nlp/PLOD-CW-25\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hCyeGJGi17A4"
      },
      "outputs": [],
      "source": [
        "labels = [\"O\", \"B-AC\", \"B-LF\", \"I-LF\"]\n",
        "n_labels = len(labels)\n",
        "ltoi = {l: i for i, l in enumerate(labels)}\n",
        "itol = {i: l for l, i in ltoi.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ub4Xj1emF83",
        "outputId": "54e1af23-21ac-4256-c684-47cde0321679"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/user/HS400/rl01179/.local/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"microsoft/deberta-v3-base\" # Or deberta-v3-large, etc.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Check if the tokenizer is a fast tokenizer (it should be for DeBERTa-v3)\n",
        "assert tokenizer.is_fast, \"Only fast tokenizers are supported for this example.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oi_28FRemLiH"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True, # Crucial for pre-tokenized input\n",
        "        max_length=512,\n",
        "        padding=\"max_length\"\n",
        "\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for i, label_sequence in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None: # Special tokens ([CLS], [SEP])\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx: # First token of a new word\n",
        "                label_ids.append(ltoi[label_sequence[word_idx]])\n",
        "            else: # Subsequent tokens of the same word\n",
        "                label_ids.append(-100)\n",
        "\n",
        "\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33SKzjqI2-5t",
        "outputId": "2ea73f15-721f-462f-fcb7-8442255988f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1, 4146, 519, 58917, 407, 107679, 2]]\n",
            "[None, 0, 1, 2, 2, 3, None]\n"
          ]
        }
      ],
      "source": [
        "t = tokenizer([\"Pakistan got nuked lmao\"])\n",
        "print(t.input_ids)\n",
        "print(t.word_ids(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "FvgTr0TQ2kiY"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "deb568b225ca4ef688e0afeda9704ffb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/112652 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70e5847e43d2408eb6bc19245ab5d86f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/24140 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bff347bced9a4447b5b3c798909343b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/24140 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data = dataset.map(tokenize_and_align_labels, batched = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AyYEqraW4PxD"
      },
      "outputs": [],
      "source": [
        "train_data, train_labels, train_attention_mask = data['train']['input_ids'], data['train']['labels'], data['train']['attention_mask']\n",
        "val_data, val_labels, val_attention_mask = data['validation']['input_ids'], data['validation']['labels'], data['validation']['attention_mask']\n",
        "test_data, test_labels, test_attention_mask = data['test']['input_ids'], data['test']['labels'], data['test']['attention_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUuL3NNq4x2S"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 8\n",
        "\n",
        "def get_batch(split = \"train\"):\n",
        "  data = train_data if split == \"train\" else val_data\n",
        "  labels = train_labels if split == \"train\" else val_labels\n",
        "  attention_mask = train_attention_mask if split == \"train\" else val_attention_mask\n",
        "  ix = torch.randint(len(data), (batch_size,))\n",
        "  x = torch.stack([torch.tensor(data[i]).long() for i in ix])\n",
        "  y = torch.stack([torch.tensor(labels[i]).long() for i in ix])\n",
        "  a = torch.stack([torch.tensor(attention_mask[i]) for i in ix])\n",
        "  return x.to(device), y.to(device), a.to(device)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(eval_steps):\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in [\"train\", \"validation\"]:\n",
        "    losses = torch.zeros(eval_steps)\n",
        "    for k in range(eval_steps):\n",
        "      x, y, a = get_batch(split)\n",
        "      logits = model(x, attention_mask = a).logits\n",
        "      loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), y.view(-1))\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.set_float32_matmul_precision(\"high\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9-2-KcIpMGK",
        "outputId": "825c5e14-26d8-4a11-867f-7178e4837b41"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=n_labels,\n",
        "    id2label=itol,\n",
        "    label2id=ltoi\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = torch.compile(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEMcSmbW4y6j",
        "outputId": "cfd7ecc3-f7be-4a37-80a5-2c1fdcae8058"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/1000 [00:13<3:47:27, 13.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 1.6057, val loss 1.6079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 101/1000 [00:47<1:05:57,  4.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 100: train loss 0.2498, val loss 0.3180\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 201/1000 [01:22<59:14,  4.45s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 200: train loss 0.1894, val loss 0.2699\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 301/1000 [01:56<52:01,  4.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 300: train loss 0.1733, val loss 0.2426\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 401/1000 [02:31<44:41,  4.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 400: train loss 0.1573, val loss 0.2549\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 501/1000 [03:05<37:15,  4.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 500: train loss 0.1544, val loss 0.2577\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 601/1000 [03:40<29:49,  4.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 600: train loss 0.1415, val loss 0.2461\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 701/1000 [04:15<22:22,  4.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 700: train loss 0.1316, val loss 0.2748\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 801/1000 [04:50<14:54,  4.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 800: train loss 0.1123, val loss 0.2540\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 901/1000 [05:24<07:25,  4.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 900: train loss 0.1190, val loss 0.2399\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [05:45<00:00,  2.90it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
        "max_steps = 1000\n",
        "\n",
        "\n",
        "for step in tqdm(range(max_steps)):\n",
        "  x, y, a = get_batch(\"train\")\n",
        "  optim.zero_grad()\n",
        "  with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
        "    logits = model(x, attention_mask = a).logits\n",
        "    loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), y.view(-1))\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "  if step % 100 == 0:\n",
        "    losses = estimate_loss(100)\n",
        "    print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['validation']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "07M0Elr_5-ZE"
      },
      "outputs": [],
      "source": [
        "from seqeval.metrics import f1_score, recall_score, precision_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_model(split=\"test\"):\n",
        "    \"\"\"Evaluate model performance on given split with seqeval metrics\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    if split == \"test\":\n",
        "        data_input_ids = test_data\n",
        "        data_labels = test_labels\n",
        "        data_attention_mask = test_attention_mask\n",
        "    elif split == \"validation\":\n",
        "        data_input_ids = val_data\n",
        "        data_labels = val_labels\n",
        "        data_attention_mask = val_attention_mask\n",
        "    else:\n",
        "        data_input_ids = train_data\n",
        "        data_labels = train_labels\n",
        "        data_attention_mask = train_attention_mask\n",
        "    \n",
        "    # Process in smaller batches to avoid OOM\n",
        "    batch_size_eval = 16\n",
        "    all_true_labels = []\n",
        "    all_pred_labels = []\n",
        "    \n",
        "    # Process the entire dataset\n",
        "    for i in tqdm(range(0, len(data_input_ids), batch_size_eval), desc=f\"Evaluating on {split}\"):\n",
        "        # Get batch\n",
        "        batch_input_ids = torch.tensor(data_input_ids[i:i+batch_size_eval]).to(device)\n",
        "        batch_labels = torch.tensor(data_labels[i:i+batch_size_eval]).to(device)\n",
        "        batch_attention_mask = torch.tensor(data_attention_mask[i:i+batch_size_eval]).to(device)\n",
        "        \n",
        "        # Get predictions\n",
        "        outputs = model(batch_input_ids, attention_mask=batch_attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        \n",
        "        # Convert predictions and labels to lists for seqeval\n",
        "        for j in range(len(batch_input_ids)):\n",
        "            true_label_ids = batch_labels[j].cpu().numpy()\n",
        "            pred_label_ids = predictions[j].cpu().numpy()\n",
        "            \n",
        "            # Convert IDs to labels, handling special tokens\n",
        "            true_seq = []\n",
        "            pred_seq = []\n",
        "            \n",
        "            for true_id, pred_id, mask in zip(true_label_ids, pred_label_ids, batch_attention_mask[j]):\n",
        "                if mask == 1 and true_id != -100:  # Only evaluate on non-padding and non-special tokens\n",
        "                    true_seq.append(itol[true_id.item()])\n",
        "                    pred_seq.append(itol[pred_id.item()])\n",
        "            \n",
        "            if true_seq:  # Only add if not empty\n",
        "                all_true_labels.append(true_seq)\n",
        "                all_pred_labels.append(pred_seq)\n",
        "    \n",
        "    # Calculate metrics using seqeval\n",
        "    precision = precision_score(all_true_labels, all_pred_labels)\n",
        "    recall = recall_score(all_true_labels, all_pred_labels)\n",
        "    f1 = f1_score(all_true_labels, all_pred_labels)\n",
        "    report = classification_report(all_true_labels, all_pred_labels)\n",
        "    \n",
        "    print(f\"\\n=== Evaluation on {split} split ===\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(\"\\nDetailed Classification Report:\")\n",
        "    print(report)\n",
        "    \n",
        "    return {\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"report\": report\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating on test: 100%|██████████| 16/16 [00:03<00:00,  4.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Evaluation on test split ===\n",
            "F1 Score: 0.8737\n",
            "Precision: 0.8259\n",
            "Recall: 0.9273\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AC       0.86      0.94      0.90       797\n",
            "          LF       0.77      0.90      0.83       482\n",
            "\n",
            "   micro avg       0.83      0.93      0.87      1279\n",
            "   macro avg       0.82      0.92      0.87      1279\n",
            "weighted avg       0.83      0.93      0.87      1279\n",
            "\n"
          ]
        }
      ],
      "source": [
        "out = evaluate_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
